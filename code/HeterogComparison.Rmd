---
title: "Heterogeneity Comparison"
author: "Ashley Naimi & Hannah Mandle"
date: "`r Sys.Date()`"
output: html_document
---

This markdown document contains the code needed to reproduce results for the manuscript entitled "...". Here, we deploy causal forests and the DR learner to estimate the ATE and the CATE for the relation between ever smoking status and a measure of ovarian reserve in 775 women from The Environment and Reproductive Health (EARTH) Study. We start by installing and loading relevant libraries:

```{r load-packages}

## Load relevant packages from CRAN

pacman::p_load(
  rio,          
  here,         
  skimr,        
  tidyverse,     
  gtsummary,
  parallel,
  rstatix,      
  janitor,      
  scales,        
  dplyr,
  tidyverse,
  haven,
  lmtest,
  sandwich,
  MASS,
  grf,
  SuperLearner,
  pROC,
  gridExtra,
  vip
  )

# set the theme for figures
thm <- theme_classic() +
  theme(
    legend.position = "top",
    legend.background = element_rect(fill = "transparent", colour = NA),
    legend.key = element_rect(fill = "transparent", colour = NA)
  )
theme_set(thm)

```

Next, we load the EARTH data, named `afc_clean`, and construct and code relevant variables:

```{r  load-and-clean-data}

#Load afc data, double check variables + attributes

new_afc <- read_csv(here("data","afc_clean.csv"))

# the relevant data:

the_data <- c("AFC_t", "DOR", 
              "age", "bmi", "white", "educ1", "eversmok", "previousIUI", "previousIVF", "gravid",
              "mEP1_sg", "mBZP1_sg", "mCNP_sg","miBP_sg",
              "mBP_sg","BP_3_sg", "M_PB_sg", "dehp_sg")

skimr::skim(new_afc[,the_data])

# create exposure variable
#exposure <- as.matrix(new_afc[,"eversmok"])
exposure <- as.matrix(new_afc[,"DOR"])

# create outcome variable
outcome <- as.matrix(new_afc[,"AFC_t"])

# identify and transform categorical
categorical_vars <- new_afc[,c("white", "educ1", "eversmok", "gravid", "previousIUI", "previousIVF")]

categorical_vars <- categorical_vars %>% 
  mutate(educ1 = factor(educ1))

# identify and transform continuous
continuous_vars <- new_afc[,c("age", "bmi",
                              "mEP1_sg", "mBZP1_sg", "mCNP_sg","miBP_sg", 
                              "mBP_sg","BP_3_sg", "M_PB_sg", "dehp_sg")]

continuous_vars <- continuous_vars %>%
  mutate_all(~(scale(.) %>% as.vector))

new_afc <- data.frame(outcome, exposure, categorical_vars, continuous_vars)

head(new_afc)

```

We'll now create the design matrices needed to conduct the analyses:

```{r feature-coding}

# relevant covariate set
covariates <- c(names(continuous_vars), names(categorical_vars))

# use covariate set to construct design matrix that doesn't include exposure
covariates_matrix <- data.frame(model.matrix(formula(paste0("~", paste0(covariates, collapse="+"))), data=new_afc)[,-1])

# use covariate set to construct design matrix that includes exposure
covariates_matrix_w <- data.frame(model.matrix(formula(paste0("~", paste0(covariates, collapse="+"), "+ DOR" )), data=new_afc)[,-1])

```

## Average Treatment Effects

We're now ready to quantify the ATEs using the causal forest algorithm and the AIPW estimator. We'll start with the causal forest approach:

```{r causal-forest}

# for reproducibility, we set the seed
set.seed(123)

# construct a sample size variable 
n <- nrow(new_afc)

# We use 10-fold cross fitting, and determine precisely which of the 775 women get assigned to each fold:
num.folds <- 10
folds <- sort(seq(n) %% num.folds) + 1

# we construct a causal forest using the `causal_forest` function from the grf package:
forest <- causal_forest(X = covariates_matrix, 
                        Y = outcome, 
                        W = exposure, 
                        num.trees = 2000,
                        honesty = TRUE,
                        min.node.size = 10,
                        alpha = .05, 
                        imbalance.penalty = 0,
                        stabilize.splits = TRUE,
                        tune.parameters = "all",
                        tune.num.trees = 200,
                        tune.num.reps = 50,
                        tune.num.draws = 1000,
                        compute.oob.predictions = TRUE,
                        num.threads = 10,
                        seed = 123,
                        clusters = folds)
```

Let's look at the variable importance from the forest:

```{r varimp-forest}

VarImpRf <- data.frame(Variable = names(covariates_matrix), 
                       Importance = variable_importance(forest))


VarImpRf %>% arrange(desc(Importance))

names(VarImpRf) <- c("Variable", "Importance")

```


We can evaluate the performance of the random forest algorithm used to construct the ATE estimate via diagnostic plotting:

```{r diag-plots-forest}
plot_dat_mu <- tibble(Predicted = forest$Y.hat,
                   Observed = outcome)

rf_mu_fit_plot <- ggplot(plot_dat_mu) + 
  geom_point(aes(x = Predicted, 
                 y = Observed)) +
  scale_x_continuous(expand = c(0,0), limits = c(0,60)) +
  scale_y_continuous(expand = c(0,0), limits = c(0,60)) +
  ggtitle("Outcome Model Predictions")

a <- roc(factor(exposure), forest$W.hat, direction="auto") 
plot_dat_pi <- data.frame(sens=a$sensitivities,spec=a$specificities)

rf_pi_fit_plot <- ggplot(plot_dat_pi) +
  geom_step(aes(1-spec,sens),linewidth=.75) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) + 
  labs(x = "1 - Specificity",y = "Sensitivity") +
  geom_abline(intercept=0,slope=1,col="gray")  +
  ggtitle("Exposure Model Predictions")

rf_diag_plot <- grid.arrange(rf_mu_fit_plot,
             rf_pi_fit_plot,
             ncol = 2)

```

This figure shows fairly poor performance of the random forest algorithms for both the outcome and propensity score models. 

It's also important to evaluate propensity score overlap:

```{r ps-overlap-forest}

plot_dat_ps <- tibble(`Propensity Score` = forest$W.hat,
                      Exposure = factor(exposure))

rf_ps_overlap_plot <- ggplot(plot_dat_ps) + 
  geom_density(aes(x = `Propensity Score`, 
                   group = Exposure, 
                   fill = Exposure), alpha = .5) +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  ggtitle("Propensity Score Overlap")

rf_ps_overlap_plot

```

Once these diagnostics are complete, we can construct an estimate of the ATE using the predictions from the random forest fit in the causal forest algorithm. 

```{r cf-ate}

# we then use the `average_treatment_effect()` function to obtain an ATE estimate
cf_ate <- average_treatment_effect(forest) # this code shows that it's using AIPW or TMLE to get ATE.

# this treatment effect is obtained via random forests using singly robust estimator
cf_ate <- cbind(t(cf_ate), 
      cf_ate[1] - 1.96*cf_ate[2],
      cf_ate[1] + 1.96*cf_ate[2])

colnames(cf_ate)[3:4] <- c("LCL", "UCL")

cf_ate

```

Next, we'll construct an AIPW estimator for the ATE using stacking (SuperLearner) as the machine learning algorithm of interest. In this case, we'll fit the super learner algorithm for the propensity score model and for the outcome model. We'll also use the same folds as above for sample splitting:

```{r super-learner-run}

mean_learner <- "SL.mean"
glm_learner <- "SL.glm"

# ranger learner
ranger_learner <- create.Learner("SL.ranger",
                                 params = list(num.trees = 500, 
                                               min.node.size = 50),
                                 tune = list(mtry = c(3,4,5)))

# glmnet learner
glmnet_learner <- create.Learner("SL.glmnet",
                                 tune = list(alpha = seq(0,1,.25)))

# xgboost learner
xgboost_learner <- create.Learner("SL.xgboost",
                                 params = list(nrounds = 500),
                                 tune = list(max_depth = c(4,6),
                                             eta = c(.1,.2)))

# earth learner
earth_learner <- create.Learner("SL.earth",
                                tune = list(degree = c(3,4,5)))

.SL.require <- function(package, message = paste('loading required package (', package, ') failed', sep = '')) {
  if(!requireNamespace(package, quietly = FALSE)) {
    stop(message, call. = FALSE)
  }
  invisible(TRUE)
}
screen.glmnet1 <- function (Y, X, family, alpha = 1, minscreen = 1, nfolds = 10, 
    nlambda = 100, ...) 
{
    .SL.require("glmnet")
    if (!is.matrix(X)) {
        X <- model.matrix(~-1 + ., X)
    }
    fitCV <- glmnet::cv.glmnet(x = X, y = Y, lambda = NULL, type.measure = "deviance", 
        nfolds = nfolds, family = family$family, alpha = alpha, 
        nlambda = nlambda)
    whichVariable <- (as.numeric(coef(fitCV$glmnet.fit, s = fitCV$lambda.min))[-1] != 
        0)
    if (sum(whichVariable) < minscreen) {
        warning("fewer than minscreen variables passed the glmnet screen, increased lambda to allow minscreen variables")
        sumCoef <- apply(as.matrix(fitCV$glmnet.fit$beta), 2, 
            function(x) sum((x != 0)))
        newCut <- which.max(sumCoef >= minscreen)
        whichVariable <- (as.matrix(fitCV$glmnet.fit$beta)[, 
            newCut] != 0)
    }
    return(whichVariable)
}

sl_lib <- c(ranger_learner$names,
            glmnet_learner$names, 
            xgboost_learner$names, 
            earth_learner$names,
            list(mean_learner,
                 glm_learner, 
                 # c(ranger_learner$names[1], "screen.glmnet1"),
                 # c(ranger_learner$names[2], "screen.glmnet1"),
                 # c(ranger_learner$names[3], "screen.glmnet1"),
                 c(glmnet_learner$names[1], "screen.glmnet1"),
                 c(glmnet_learner$names[2], "screen.glmnet1"),
                 c(glmnet_learner$names[3], "screen.glmnet1"),
                 c(glmnet_learner$names[4], "screen.glmnet1"),
                 c(glmnet_learner$names[5], "screen.glmnet1"),
                 c(xgboost_learner$names[1], "screen.glmnet1"),
                 c(xgboost_learner$names[2], "screen.glmnet1"),
                 c(xgboost_learner$names[3], "screen.glmnet1"),
                 c(xgboost_learner$names[4], "screen.glmnet1"),
                 c(earth_learner$names[1], "screen.glmnet1"),
                 c(earth_learner$names[2], "screen.glmnet1"),
                 c(earth_learner$names[3], "screen.glmnet1")))

# Specify the number of folds for V-fold cross-validation
# Use same folds as used for causal_forest function
# Doing cross-validation this way automatically deploys cross-fitting
fold_dat <- tibble(id = 1:n,folds)
fold_index <- split(fold_dat$id,fold_dat$folds)

# we'll use parallel processing to speed things up
options(mc.cores = detectCores() - 2)

getOption("mc.cores")

env_vars <- covariates_matrix_w %>% dplyr::select(ends_with("_sg"))

augment_data <- env_vars*covariates_matrix_w$DOR
names(augment_data) <- paste0(names(augment_data),"_DOR")

covariates_matrix_w_augment <- cbind(covariates_matrix_w, augment_data)

fit_mu <- CV.SuperLearner(Y = outcome,
                          X = covariates_matrix_w_augment, 
                          method = "method.NNLS", 
                          family = gaussian,
                          SL.library = sl_lib,
                          cvControl = list(V = num.folds, validRows = fold_index),
                          control = list(saveCVFitLibrary = T),
                          parallel = "multicore",
                          verbose = T)

fit_pi <- CV.SuperLearner(Y = exposure,
                          X = covariates_matrix,
                          method = "method.NNLS", 
                          family = binomial,
                          SL.library = sl_lib,
                          cvControl = list(V = num.folds, validRows = fold_index),
                          control = list(saveCVFitLibrary = T),
                          parallel = "multicore",
                          verbose = T)
```

Let's explore the fit for each of these stacked algorithms. First, we'll examine the stacking results for the model fits:

```{r sl-fits}

summary(fit_mu)

summary(fit_pi)

```

We can also construct permutation based variable importance measures for the outcome model with superlearner using the `vip` package:

```{r varimp-superlearner}

fit_mu_2 <- mcSuperLearner(Y = outcome,
                           X = covariates_matrix_w_augment, 
                           method = "method.NNLS", 
                           family = gaussian,
                           SL.library = sl_lib,
                           cvControl = list(V = num.folds, validRows = fold_index),
                           control = list(saveCVFitLibrary = T),
                           verbose = T)

pred_wrapper <- function(sl, newdata) {
  predict(sl, newdata, onlySL=T)$pred
}

VarImpSL <- vi_permute(object = fit_mu_2, 
                 method = "permute", 
                 feature_names = names(covariates_matrix), 
                 train = covariates_matrix_w_augment, 
                 target = outcome,
                 metric = "RMSE",
                 type = "difference", 
                 nsim = 5, 
                 pred_wrapper = pred_wrapper) 

names(VarImpSL) <- c("Variable", "Importance", "SD")

VarImpSL <- VarImpSL[,c("Variable","Importance")]

```


Let's compare the variable importance measures from each approach:

```{r varimp}

VarImpSL$Model <- "Super Learner"
VarImpRf$Model <- "Causal Forest"

VarImp <- rbind(VarImpRf, VarImpSL)

mean_for_rank <- VarImp %>% 
  group_by(Variable) %>% 
  summarize(meanVI = mean(Importance))

VarImp <- left_join(VarImp, mean_for_rank, by = "Variable")
var_names <- unique(VarImp[order(-VarImp$meanVI),]$Variable)
var_names <- var_names[grepl("_sg", var_names)][1:4] # to save for later...

ggplot(VarImp) + 
  geom_bar(aes(x = reorder(Variable, -meanVI), 
               y = Importance, 
               fill = Model),
           position="dodge", 
           stat="identity") +
  theme(axis.text.x = element_text(angle = 70, 
                                   vjust = 0.5, 
                                   hjust=.5)) +
  scale_y_continuous(expand = c(0,0))

## create separate variable list for cf versus drl

var_names_rf <- unique(VarImpRf[order(-VarImpRf$Importance),]$Variable)
var_names_rf <- var_names_rf[grepl("_sg", var_names_rf)][1:4] # to save for later...

var_names_sl <- unique(VarImpSL[order(-VarImpSL$Importance),]$Variable)
var_names_sl <- var_names_sl[grepl("_sg", var_names_sl)][1:4] # to save for later...


```

We'll also construct the same diagnostics for the super learner models. 

```{r diag-plots-sl}

plot_dat_mu <- tibble(Predicted = fit_mu$SL.predict,
                      Observed = outcome)

sl_mu_fit_plot <- ggplot(plot_dat_mu) + 
  geom_point(aes(x = Predicted, 
                 y = Observed)) +
  scale_x_continuous(expand = c(0,0), limits = c(0,60)) +
  scale_y_continuous(expand = c(0,0), limits = c(0,60)) +
  ggtitle("Outcome Model Predictions")

a <- roc(factor(exposure), fit_pi$SL.predict, direction="auto") 
plot_dat_pi <- data.frame(sens=a$sensitivities,spec=a$specificities)

sl_pi_fit_plot <- ggplot(plot_dat_pi) +
  geom_step(aes(1-spec,sens),linewidth=.75) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) + 
  labs(x = "1 - Specificity",y = "Sensitivity") +
  geom_abline(intercept=0,slope=1,col="gray")  +
  ggtitle("Exposure Model Predictions")

sl_diag_plot <- grid.arrange(sl_mu_fit_plot,
             sl_pi_fit_plot,
             ncol = 2)

```

Here is the propensity score overlap plot for the super learner model as well:

```{r ps-overlap-sl}

plot_dat_ps <- tibble(`Propensity Score` = fit_pi$SL.predict,
                      Exposure = factor(exposure))

sl_ps_overlap_plot <- ggplot(plot_dat_ps) + 
  geom_density(aes(x = `Propensity Score`, 
                   group = Exposure, 
                   fill = Exposure), alpha = .5) +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  ggtitle("Propensity Score Overlap")

sl_ps_overlap_plot

```

Again, with these diagnostics complete, we can construct and ATE estimate using the double robust AIPW estimator, with the exposure and outcome models obtained via stacking:

```{r aipw-ate}

## cross-fit predictions

pscore <- as.matrix(fit_pi$SL.predict)

mu_hat <- as.matrix(fit_mu$SL.predict)

mu_hat1 <- NULL
for(i in 1:num.folds){
mu_hat1 <- rbind(mu_hat1, 
                 predict(fit_mu$AllSL[[i]],
                   newdata = base::transform(
                     covariates_matrix_w_augment[fold_index[[i]],], DOR = 1), 
                           onlySL=T)$pred)
}

mu_hat0 <- NULL
for(i in 1:num.folds){
mu_hat0 <- rbind(mu_hat0, 
                 predict(fit_mu$AllSL[[i]],
                   newdata = base::transform(
                     covariates_matrix_w_augment[fold_index[[i]],], DOR = 0), 
                           onlySL=T)$pred)
}

## aipw
aipw_func <- function(exposure, outcome, pscore, mu_hat, mu_hat0, mu_hat1){
  aipw_score <- ((2*exposure - 1)*(outcome - mu_hat))/((2*exposure - 1)*pscore + (1 - exposure)) + (mu_hat1 - mu_hat0)
  return(aipw_score)
}

aipw_score <- aipw_func(exposure, 
                        outcome, 
                        pscore, 
                        mu_hat, 
                        mu_hat0, 
                        mu_hat1)
colnames(aipw_score) <- NULL

aipw_psi <- mean(aipw_score)

aipw_se <- sd(aipw_score)/sqrt(n)

aipw_ate <- c(aipw_psi, aipw_se)

aipw_ate <- cbind(t(aipw_ate), 
      aipw_ate[1] - 1.96*aipw_ate[2],
      aipw_ate[1] + 1.96*aipw_ate[2])

colnames(aipw_ate) <- c("estimate", "std.err", "LCL", "UCL")

```

Our first set of results are basically estimates of the average treatment effect using singly robust causal forests, and double robust AIPW. Both approaches deployed a form of cross-fitting, so that the same observations were not used to fit the models and then construct the estimates.

The overall ATE results are as follows:

```{r overall-ate}

res_ate <-  rbind(cf_ate,
                  aipw_ate)

row.names(res_ate) <- c("Causal Forest", "AIPW with Stacking")

res_ate

```

## Conditional Average Treatment Effects

Variable importance measures from both models above suggest `r var_names` as important environmental covariates for determining splits in the causal forest algorithm, and explaining the super learner outcome variability. Here, we explore how the ATE varies as a function of these covariates. We'll start with the causal forest approach:

```{r rf-calibration}

tau.hat = predict(forest)$predictions

# Compare regions with high and low estimated CATEs
high_effect = tau.hat > median (tau.hat)
ate.high = average_treatment_effect(forest, subset = high_effect)
ate.low = average_treatment_effect(forest, subset = !high_effect)

ate.high
ate.low

# Run best linear predictor analysis
test_calibration(forest)

```

If `mean.forest.prediction` = 1, then the average prediction produced by the forest is correct. Meanwhile, if `differential.forest.prediction` = 1, then the forest predictions adequately capture the underlying heterogeneity.

The slope `differential.forest.prediction` is a measure of how the CATE predictions covary with true CATE. Therefore, the p-value on the estimate of coefficient also acts as an omnibus test for the presence of heterogeneity. If the coefficient is significantly greater than zero, then we can reject the null of no heterogeneity. However, coefficients smaller than 0 are not meaningful and should not be interpreted.


```{r rf-blp}

best_linear_projection(forest, covariates_matrix[,c(var_names_rf)])

```

Let's look at a similar best linear projection with the DR learner approach

```{r sl-blp}
score_dat <- data.frame(aipw_score, covariates_matrix[,c(var_names_sl)])
score_fit <- lm(aipw_score ~ ., data = score_dat)
coeftest(score_fit, vcov = vcovHC(score_fit, type = "HC3"))

```



```{r dr-learner-dose-response-forest}

ss10 <- smooth.spline(covariates_matrix$dehp_sg, 
                      covariates_matrix$aipw_score, 
                      df = 2)
plot(ss10)
lines(ss10, lty = 2, col = "red")
ss10 <- smooth.spline(covariates_matrix$dehp_sg, 
                      covariates_matrix$forest_score, 
                      df = 2)
lines(ss10, lty = 2, col = "blue")

```

```{r dr-learner-dose-response-aipw}

ss10 <- smooth.spline(covariates_matrix$dehp_sg, 
                      covariates_matrix$aipw_score, 
                      df = 2)
plot(ss10)
lines(ss10, lty = 2, col = "red")
ss10 <- smooth.spline(covariates_matrix$dehp_sg, 
                      covariates_matrix$forest_score, 
                      df = 2)
lines(ss10, lty = 2, col = "blue")

```